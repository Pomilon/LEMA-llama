{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEMA Inference & Merging Demonstration\n",
        "This notebook loads the fine-tuned LEMA model (adapter weights) and runs inference to verify the custom chat format.\n",
        "It also demonstrates how to merge the adapter into the base model if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers safetensors accelerate\n",
        "# Clone LEMA repository\n",
        "!git clone https://github.com/Pomilon/LEMA.git\n",
        "!pip install -q -e LEMA/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('inference/framework', exist_ok=True)\n",
        "os.makedirs('inference/engines', exist_ok=True)\n",
        "os.makedirs('inference', exist_ok=True)\n",
        "os.makedirs('checkpoints/final', exist_ok=True)\n",
        "\n",
        "# Create __init__.py\n",
        "with open('inference/__init__.py', 'w') as f: pass\n",
        "with open('inference/framework/__init__.py', 'w') as f: pass\n",
        "with open('inference/engines/__init__.py', 'w') as f: pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os; os.makedirs('inference/framework', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile inference/framework/model_handler.py\n",
        "\"\"\"\n",
        "Handle loading and interaction with LEMA fine-tuned models.\n",
        "\"\"\"\n",
        "\n",
        "from lema import LemaModel, MemoryStrategy\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import threading\n",
        "from typing import Optional, List\n",
        "import time\n",
        "\n",
        "class LemaModelHandler:\n",
        "    \"\"\"Manages LEMA model loading and inference.\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_path: str, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Load a fine-tuned LEMA model.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to LEMA checkpoint\n",
        "            device: Device to run on (cuda/cpu)\n",
        "        \"\"\"\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.device = device\n",
        "        \n",
        "        print(f\"Loading LEMA model from {checkpoint_path}...\")\n",
        "        # Load model using LEMA's API\n",
        "        self.model = LemaModel.from_pretrained(checkpoint_path, device=device)\n",
        "        # Ensure model components are on correct device\n",
        "        self.model.to(device)\n",
        "        self.model.initialize_lora()\n",
        "        \n",
        "        # Access internal components for manual forward pass\n",
        "        self.memory = self.model.memory\n",
        "        self.adapter = self.model.adapter\n",
        "        self.layers = self.adapter.get_layer_metadata()\n",
        "        self.lora_manager = self.model.lora_manager\n",
        "        \n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model.config.model_name_or_path\n",
        "        )\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "        # Debug: Check LoRA weights\n",
        "        lora_params = self.model.get_trainable_parameters()\n",
        "        if lora_params:\n",
        "            total_norm = sum(p.norm().item() for p in lora_params)\n",
        "            print(f\"Debug: Total LoRA weight norm: {total_norm:.4f}\")\n",
        "            if total_norm == 0:\n",
        "                print(\"\u26a0\ufe0f Warning: LoRA weights are all zeros!\")\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f Warning: No LoRA parameters found in model!\")\n",
        "\n",
        "        print(\"Model loaded successfully.\")\n",
        "    \n",
        "    def generate(\n",
        "        self, \n",
        "        prompt: str, \n",
        "        max_new_tokens: int = 128,\n",
        "        temperature: float = 0.7,\n",
        "        top_p: float = 0.9,\n",
        "        do_sample: bool = True,\n",
        "        stop_sequences: List[str] = [\"[/LEMA_REPLY]\"]\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate text from prompt using LEMA's streaming architecture.\n",
        "        \"\"\"\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            prompt, \n",
        "            return_tensors=\"pt\", \n",
        "            truncation=True,\n",
        "            max_length=512 # limit input context\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "        \n",
        "        # Generate loop\n",
        "        current_input_ids = input_ids\n",
        "        \n",
        "        print(\"Generating...\", end=\"\", flush=True)\n",
        "        \n",
        "        for i in range(max_new_tokens):\n",
        "            with torch.no_grad():\n",
        "                logits = self._forward_pass(current_input_ids)\n",
        "            \n",
        "            # Get last token logits\n",
        "            next_token_logits = logits[0, -1, :]\n",
        "            \n",
        "            # Apply temperature\n",
        "            if temperature > 0:\n",
        "                next_token_logits = next_token_logits / temperature\n",
        "            \n",
        "            # Sample or greedy\n",
        "            if do_sample and temperature > 0:\n",
        "                # Top-p (nucleus) sampling\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "                    \n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "                    \n",
        "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                    next_token_logits[indices_to_remove] = -float('inf')\n",
        "                \n",
        "                probs = torch.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "            \n",
        "            # Append\n",
        "            current_input_ids = torch.cat([current_input_ids, next_token.unsqueeze(0)], dim=1)\n",
        "            \n",
        "            # Check EOS\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "                \n",
        "            print(\".\", end=\"\", flush=True)\n",
        "\n",
        "            # Check stop sequences\n",
        "            if stop_sequences:\n",
        "                decoded_so_far = self.tokenizer.decode(current_input_ids[0, -20:], skip_special_tokens=False)\n",
        "                if any(stop in decoded_so_far for stop in stop_sequences):\n",
        "                    break\n",
        "        \n",
        "        print(\" Done!\")\n",
        "        \n",
        "        # Decode\n",
        "        output_text = self.tokenizer.decode(current_input_ids[0], skip_special_tokens=False)\n",
        "        return output_text\n",
        "    \n",
        "    def _forward_pass(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Custom forward pass logic for LEMA models.\n",
        "        Replicates LemaTrainer.train_step logic but inference only.\n",
        "        \"\"\"\n",
        "        is_streaming = (self.model.config.strategy == MemoryStrategy.STREAMING)\n",
        "        hidden_states = input_ids # Start with embeddings usually handled by first layer or similar?\n",
        "        # Wait, LemaModelAdapter usually handles embeddings in the first layer or separately.\n",
        "        # In LemaTrainer: hidden_states = inputs (which are input_ids)\n",
        "        # So the adapter handles input_ids -> embeddings.\n",
        "        \n",
        "        # Prefetch first layer\n",
        "        if is_streaming:\n",
        "            self.memory.prefetch_to_ram(self.layers[0]['id'], 0)\n",
        "            self.memory.async_transfer_to_vram(self.layers[0]['id'], 0, ram_slot=0)\n",
        "            if len(self.layers) > 1:\n",
        "                self.memory.prefetch_to_ram(self.layers[1]['id'], 1)\n",
        "        else:\n",
        "            self.memory.async_transfer_to_vram(self.layers[0]['id'], 0)\n",
        "            \n",
        "        for i, layer_meta in enumerate(self.layers):\n",
        "            slot = i % 2\n",
        "            next_slot = (i + 1) % 2\n",
        "            \n",
        "            flat_vram = self.memory.get_vram_flat_buffer(slot)\n",
        "            \n",
        "            # Prefetch next layers\n",
        "            disk_thread = None\n",
        "            if i + 1 < len(self.layers):\n",
        "                if is_streaming:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i+1]['id'], next_slot, ram_slot=next_slot)\n",
        "                    if i + 2 < len(self.layers):\n",
        "                        disk_thread = threading.Thread(target=self.memory.prefetch_to_ram, args=(self.layers[i+2]['id'], slot))\n",
        "                        disk_thread.start()\n",
        "                else:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i+1]['id'], next_slot)\n",
        "            \n",
        "            # Construct layer\n",
        "            layer_module = self.adapter.construct_layer_module(layer_meta['id'], flat_vram, self.lora_manager)\n",
        "            \n",
        "            # Forward\n",
        "            # Note: We disable gradient checkpointing for inference\n",
        "            hidden_states = self.adapter.forward_layer(layer_module, hidden_states, gradient_checkpointing=False)\n",
        "            \n",
        "            if disk_thread: disk_thread.join()\n",
        "            \n",
        "            # Release layer (move to CPU/Disk if needed, or just free VRAM pointer)\n",
        "            # LemaModelAdapter.release_layer_module handles cleanup\n",
        "            if hasattr(self.adapter, \"release_layer_module\"):\n",
        "                self.adapter.release_layer_module(layer_module)\n",
        "            del layer_module\n",
        "            \n",
        "        return hidden_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os; os.makedirs('inference/framework', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile inference/framework/chat_parser.py\n",
        "\"\"\"\n",
        "Parse and validate LEMA custom chat format.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from typing import Optional, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class LemaResponse:\n",
        "    \"\"\"Parsed LEMA response.\"\"\"\n",
        "    answer: str\n",
        "    explanation: str\n",
        "    confidence: str\n",
        "    raw_text: str\n",
        "    is_valid: bool\n",
        "\n",
        "class ChatParser:\n",
        "    \"\"\"Parse LEMA custom chat format.\"\"\"\n",
        "    \n",
        "    # Regex patterns for extracting fields\n",
        "    LEMA_REPLY_PATTERN = r'\\\\[LEMA_REPLY\\\\](.*?)\\\\[/LEMA_REPLY\\\\]'\n",
        "    ANSWER_PATTERN = r'Answer:\\s*(.+?)(?=\\n|Explanation:|Confidence:|$)'\n",
        "    EXPLANATION_PATTERN = r'Explanation:\\s*(.+?)(?=\\n|Confidence:|$)'\n",
        "    CONFIDENCE_PATTERN = r'Confidence:\\s*(High|Medium|Low)'\n",
        "    \n",
        "    @classmethod\n",
        "    def parse_response(cls, text: str) -> LemaResponse:\n",
        "        \"\"\"\n",
        "        Parse a LEMA-formatted response.\n",
        "        \n",
        "        Args:\n",
        "            text: Generated text that should contain [LEMA_REPLY] block\n",
        "        \n",
        "        Returns:\n",
        "            LemaResponse with parsed fields and validation status\n",
        "        \"\"\"\n",
        "        # Extract LEMA_REPLY block\n",
        "        reply_match = re.search(cls.LEMA_REPLY_PATTERN, text, re.DOTALL)\n",
        "        \n",
        "        if not reply_match:\n",
        "            return LemaResponse(\n",
        "                answer=\"\",\n",
        "                explanation=\"\",\n",
        "                confidence=\"\",\n",
        "                raw_text=text,\n",
        "                is_valid=False\n",
        "            )\n",
        "        \n",
        "        reply_content = reply_match.group(1)\n",
        "        \n",
        "        # Extract fields\n",
        "        answer_match = re.search(cls.ANSWER_PATTERN, reply_content, re.DOTALL)\n",
        "        explanation_match = re.search(cls.EXPLANATION_PATTERN, reply_content, re.DOTALL)\n",
        "        confidence_match = re.search(cls.CONFIDENCE_PATTERN, reply_content)\n",
        "        \n",
        "        answer = answer_match.group(1).strip() if answer_match else \"\"\n",
        "        explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
        "        confidence = confidence_match.group(1).strip() if confidence_match else \"\"\n",
        "        \n",
        "        is_valid = bool(answer and explanation and confidence)\n",
        "        \n",
        "        return LemaResponse(\n",
        "            answer=answer,\n",
        "            explanation=explanation,\n",
        "            confidence=confidence,\n",
        "            raw_text=text,\n",
        "            is_valid=is_valid\n",
        "        )\n",
        "    \n",
        "    @classmethod\n",
        "    def format_prompt(cls, user_message: str) -> str:\n",
        "        \"\"\"\n",
        "        Format a user message into the LEMA chat template.\n",
        "        \n",
        "        Args:\n",
        "            user_message: User's question/input\n",
        "        \n",
        "        Returns:\n",
        "            Properly formatted prompt for the model\n",
        "        \"\"\"\n",
        "        return f\"\"\"<|system|>\n",
        "You are a precise assistant trained using LEMA.\n",
        "\n",
        "<|user|>\n",
        "{user_message}\n",
        "\n",
        "<|assistant|>\n",
        "[LEMA_REPLY]\n",
        "Answer:\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os; os.makedirs('inference/framework', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile inference/framework/conversation.py\n",
        "\"\"\"\n",
        "Manage conversation state and history.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    \"\"\"Single message in conversation.\"\"\"\n",
        "    role: str  # 'user' or 'assistant'\n",
        "    content: str\n",
        "    metadata: Dict = field(default_factory=dict)\n",
        "\n",
        "class ConversationManager:\n",
        "    \"\"\"Manage conversation history and context.\"\"\"\n",
        "    \n",
        "    def __init__(self, max_history: int = 10):\n",
        "        \"\"\"\n",
        "        Initialize conversation manager.\n",
        "        \n",
        "        Args:\n",
        "            max_history: Maximum number of turns to keep\n",
        "        \"\"\"\n",
        "        self.max_history = max_history\n",
        "        self.messages: List[Message] = []\n",
        "    \n",
        "    def add_user_message(self, content: str):\n",
        "        \"\"\"Add user message to history.\"\"\"\n",
        "        self.messages.append(Message(role='user', content=content))\n",
        "        self._trim_history()\n",
        "    \n",
        "    def add_assistant_message(self, content: str, **metadata):\n",
        "        \"\"\"Add assistant message to history.\"\"\"\n",
        "        self.messages.append(Message(\n",
        "            role='assistant', \n",
        "            content=content,\n",
        "            metadata=metadata\n",
        "        ))\n",
        "        self._trim_history()\n",
        "    \n",
        "    def get_context(self, include_current: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Build context string from conversation history.\n",
        "        \n",
        "        Args:\n",
        "            include_current: Whether to include the most recent exchange\n",
        "        \n",
        "        Returns:\n",
        "            Formatted conversation context\n",
        "        \"\"\"\n",
        "        # For now, we just use the immediate question\n",
        "        # You could extend this to include conversation history\n",
        "        if not self.messages:\n",
        "            return \"\"\n",
        "        \n",
        "        # Get last user message\n",
        "        for msg in reversed(self.messages):\n",
        "            if msg.role == 'user':\n",
        "                return msg.content\n",
        "        \n",
        "        return \"\"\n",
        "    \n",
        "    def clear(self):\n",
        "        \"\"\"Clear conversation history.\"\"\"\n",
        "        self.messages.clear()\n",
        "    \n",
        "    def _trim_history(self):\n",
        "        \"\"\"Keep only max_history recent messages.\"\"\"\n",
        "        if len(self.messages) > self.max_history:\n",
        "            self.messages = self.messages[-self.max_history:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os; os.makedirs('inference/engines', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile inference/engines/cli_engine.py\n",
        "\"\"\"\n",
        "Command-line interface for LEMA chatbot.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add framework to path\n",
        "sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
        "\n",
        "from inference.framework.model_handler import LemaModelHandler\n",
        "from inference.framework.chat_parser import ChatParser\n",
        "from inference.framework.conversation import ConversationManager\n",
        "\n",
        "class CLIChatEngine:\n",
        "    \"\"\"Interactive CLI chat interface.\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_path: str, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize CLI chat engine.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to LEMA checkpoint\n",
        "            device: Device to run on\n",
        "        \"\"\"\n",
        "        print(\"Loading model...\")\n",
        "        self.model_handler = LemaModelHandler(checkpoint_path, device)\n",
        "        self.conversation = ConversationManager()\n",
        "        self.parser = ChatParser()\n",
        "        print(\"\u2705 Model loaded!\\n\")\n",
        "    \n",
        "    def run(self):\n",
        "        \"\"\"Run interactive chat loop.\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"LEMA Chatbot - Custom Format Demonstration\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nCommands:\")\n",
        "        print(\"  'quit' or 'exit' - Exit the chat\")\n",
        "        print(\"  'clear' - Clear conversation history\")\n",
        "        print(\"  'debug' - Toggle debug mode\")\n",
        "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "        \n",
        "        debug_mode = False\n",
        "        \n",
        "        while True:\n",
        "            try:\n",
        "                user_input = input(\"You: \").strip()\n",
        "                \n",
        "                if not user_input:\n",
        "                    continue\n",
        "                \n",
        "                if user_input.lower() in ['quit', 'exit']:\n",
        "                    print(\"\\nGoodbye!\")\n",
        "                    break\n",
        "                \n",
        "                if user_input.lower() == 'clear':\n",
        "                    self.conversation.clear()\n",
        "                    print(\"\\n\ud83d\udd04 Conversation cleared\\n\")\n",
        "                    continue\n",
        "                \n",
        "                if user_input.lower() == 'debug':\n",
        "                    debug_mode = not debug_mode\n",
        "                    print(f\"\\n\ud83d\udc1b Debug mode: {'ON' if debug_mode else 'OFF'}\\n\")\n",
        "                    continue\n",
        "                \n",
        "                # Add to conversation\n",
        "                self.conversation.add_user_message(user_input)\n",
        "                \n",
        "                # Format prompt\n",
        "                prompt = self.parser.format_prompt(user_input)\n",
        "                \n",
        "                if debug_mode:\n",
        "                    print(f\"\\n[DEBUG] Prompt:\\n{prompt}\\n\")\n",
        "                \n",
        "                # Generate response\n",
        "                response_text = self.model_handler.generate(prompt)\n",
        "                \n",
        "                if debug_mode:\n",
        "                    print(f\"\\n[DEBUG] Raw response:\\n{response_text}\\n\")\n",
        "                \n",
        "                # Parse response\n",
        "                parsed = self.parser.parse_response(response_text)\n",
        "                \n",
        "                if parsed.is_valid:\n",
        "                    # Display parsed response\n",
        "                    print(f\"\\nAssistant: {parsed.answer}\")\n",
        "                    print(f\"\ud83d\udca1 {parsed.explanation}\")\n",
        "                    print(f\"\ud83d\udcca Confidence: {parsed.confidence}\\n\")\n",
        "                    \n",
        "                    # Add to conversation\n",
        "                    self.conversation.add_assistant_message(\n",
        "                        parsed.answer,\n",
        "                        explanation=parsed.explanation,\n",
        "                        confidence=parsed.confidence\n",
        "                    )\n",
        "                else:\n",
        "                    # Model didn't follow format\n",
        "                    print(f\"\\n\u26a0\ufe0f  Model response didn't follow LEMA format:\")\n",
        "                    print(f\"{response_text}\\n\")\n",
        "                    print(\"This might indicate the model needs more training.\\n\")\n",
        "            \n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n\\nGoodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"\\n\u274c Error: {e}\\n\")\n",
        "                if debug_mode:\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main entry point.\"\"\"\n",
        "    import argparse\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description=\"LEMA CLI Chat Interface\")\n",
        "    parser.add_argument(\n",
        "        \"checkpoint\",\n",
        "        type=str,\n",
        "        help=\"Path to LEMA checkpoint directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--device\",\n",
        "        type=str,\n",
        "        default=\"cuda\",\n",
        "        help=\"Device to run on (cuda/cpu)\"\n",
        "    )\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    engine = CLIChatEngine(args.checkpoint, args.device)\n",
        "    engine.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os; os.makedirs('tools', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile tools/merge_adapter.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import gc\n",
        "import psutil\n",
        "import json\n",
        "import shutil\n",
        "from safetensors.torch import save_file\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Adjust path to import lema\n",
        "import sys\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "from lema import LemaConfig, LemaModel\n",
        "\n",
        "def get_ram_usage():\n",
        "    return psutil.virtual_memory().used / 1e9\n",
        "\n",
        "def get_disk_usage(path=\".\"):\n",
        "    total, used, free = shutil.disk_usage(path)\n",
        "    return free / 1e9\n",
        "\n",
        "def merge_adapter(checkpoint_dir: str, output_dir: str, base_model_path: str, repo_id: str = None, token: str = None):\n",
        "    \"\"\"\n",
        "    Merges LEMA LoRA adapter into base model.\n",
        "    If repo_id is provided, performs STREAMING UPLOAD:\n",
        "    - Saves a shard\n",
        "    - Uploads to HF\n",
        "    - Deletes local shard\n",
        "    This bypasses local disk limits.\n",
        "    \"\"\"\n",
        "    print(f\"[{get_ram_usage():.2f}GB RAM | {get_disk_usage():.2f}GB Disk] Loading LEMA config...\")\n",
        "    \n",
        "    api = None\n",
        "    if repo_id:\n",
        "        if not token:\n",
        "            print(\"\u274c Repo ID provided but no token found.\")\n",
        "            return\n",
        "        api = HfApi(token=token)\n",
        "        print(f\"\ud83d\ude80 Streaming Upload Enabled: Target -> {repo_id}\")\n",
        "    \n",
        "    try:\n",
        "        config = LemaConfig.from_pretrained(checkpoint_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading config: {e}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(config.gbi_path) and not os.path.exists(base_model_path):\n",
        "        if os.path.exists(base_model_path):\n",
        "            config.gbi_path = base_model_path\n",
        "        else:\n",
        "            print(\"Base model not found.\")\n",
        "            return\n",
        "\n",
        "    print(f\"[{get_ram_usage():.2f}GB] Initializing LEMA model...\")\n",
        "    config.device = \"cpu\"\n",
        "    model = LemaModel(config)\n",
        "    model.adapter._max_pool_size = 1\n",
        "    \n",
        "    print(f\"[{get_ram_usage():.2f}GB] Loading adapter weights...\")\n",
        "    model.lora_manager.load_pretrained(checkpoint_dir)\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Metadata for index.json\n",
        "    weight_map = {}\n",
        "    \n",
        "    # Define Shards (4 layers per shard)\n",
        "    layers = model.adapter.get_layer_metadata()\n",
        "    block_layers = [l for l in layers if l['type'] == 'block']\n",
        "    shard_size = 4\n",
        "    \n",
        "    # Calculate Total Shards for Naming\n",
        "    # Embeddings (1) + Layers (32) + Head/Norm (1) = 34 \"units\"\n",
        "    # Embeddings is processed alone -> Shard 1\n",
        "    # Layers (32) / 4 = 8 Shards\n",
        "    # Head/Norm -> Final Shard\n",
        "    # Total ~10 shards? Let's keep it dynamic but we need total count for proper naming \"00001-of-XXXXX\"\n",
        "    # Actually, safetensors naming convention \"model-00001-of-00005.safetensors\" assumes we know total at start.\n",
        "    # Let's pre-calculate.\n",
        "    # 1 (Emb) + 8 (Layers) + 1 (Head) = 10 shards.\n",
        "    \n",
        "    total_shards = 1 + (len(block_layers) // shard_size) + 1\n",
        "    if len(block_layers) % shard_size != 0: total_shards += 1 # Remainder\n",
        "    \n",
        "    current_shard_idx = 1\n",
        "    current_shard_weights = {}\n",
        "    \n",
        "    def save_and_upload_shard():\n",
        "        nonlocal current_shard_idx, current_shard_weights\n",
        "        if not current_shard_weights: return\n",
        "        \n",
        "        # Proper naming from the start\n",
        "        filename = f\"model-{current_shard_idx:05d}-of-{total_shards:05d}.safetensors\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        \n",
        "        print(f\"[{get_ram_usage():.2f}GB RAM | {get_disk_usage():.2f}GB Disk] Saving {filename}...\")\n",
        "        save_file(current_shard_weights, filepath)\n",
        "        \n",
        "        # Update map\n",
        "        for k in current_shard_weights.keys():\n",
        "            weight_map[k] = filename\n",
        "            \n",
        "        # Clear memory\n",
        "        current_shard_weights.clear()\n",
        "        current_shard_idx += 1\n",
        "        gc.collect()\n",
        "        \n",
        "        # UPLOAD AND DELETE\n",
        "        if api:\n",
        "            print(f\"\u2b06\ufe0f Uploading {filename}...\")\n",
        "            try:\n",
        "                api.upload_file(\n",
        "                    path_or_fileobj=filepath,\n",
        "                    path_in_repo=filename,\n",
        "                    repo_id=repo_id,\n",
        "                    repo_type=\"model\",\n",
        "                    commit_message=f\"Upload shard {current_shard_idx-1}/{total_shards}\"\n",
        "                )\n",
        "                print(f\"\u2705 Uploaded. Deleting local file to save space.\")\n",
        "                os.remove(filepath)\n",
        "            except Exception as e:\n",
        "                print(f\"\u274c Upload failed for {filename}: {e}\")\n",
        "                # Don't delete if upload failed, so user can manually recover if space allows\n",
        "        else:\n",
        "            print(f\"\ud83d\udcbe Saved locally.\")\n",
        "\n",
        "    # --- 1. Embeddings ---\n",
        "    print(f\"Processing Embeddings (Shard {current_shard_idx})...\")\n",
        "    emb_name = model.adapter.get_param_names_for_layer(0)[0]\n",
        "    current_shard_weights[\"model.embed_tokens.weight\"] = model.memory.gbi.handle.get_tensor(emb_name).clone().to(dtype=torch.float16)\n",
        "    save_and_upload_shard() # Save embeddings as shard 1\n",
        "\n",
        "    # --- 2. Transformer Layers ---\n",
        "    for i, layer_meta in enumerate(block_layers):\n",
        "        idx = layer_meta['block_index']\n",
        "        \n",
        "        if idx % 5 == 0:\n",
        "            print(f\"[{get_ram_usage():.2f}GB] Merging Layer {idx}...\")\n",
        "        \n",
        "        # Load & Merge\n",
                "        model.memory.prefetch_to_ram(layer_meta['id'], 0)\n",
                "        flat_buffer = model.memory.ram_buffers[0]\n",
                "        module = model.adapter.construct_layer_module(layer_meta['id'], flat_buffer, model.lora_manager)\n"
        ,
        "        \n",
        "        for _, child in module.named_modules():\n",
        "            if hasattr(child, \"lora_A\") and hasattr(child, \"base_layer\"):\n",
        "                scale = child.scaling\n",
        "                delta = (child.lora_B.data @ child.lora_A.data) * scale\n",
        "                child.base_layer.weight.data += delta.to(child.base_layer.weight.dtype)\n",
        "        \n",
        "        # Extract\n",
        "        prefix = f\"model.layers.{idx}.\"\n",
        "        names = model.adapter.get_param_names_for_layer(layer_meta['id'])\n",
        "        module_params = dict(module.named_parameters())\n",
        "        \n",
        "        for full_name in names:\n",
        "            clean_k = full_name[len(prefix):]\n",
        "            if clean_k not in module_params:\n",
        "                clean_k = clean_k.replace(\".weight\", \".base_layer.weight\")\n",
        "            \n",
        "            # Store in state dict (clone to detach from LEMA's reusable buffer)\n",
        "            # CAST TO FP16 to save space (Standard Llama is FP16/BF16)\n",
        "            current_shard_weights[full_name] = module_params[clean_k].data.clone().to(dtype=torch.float16).cpu()\n",
        "            \n",
        "        del module\n",
        "        model.adapter.layer_pool.clear()\n",
        "        gc.collect()\n",
        "        \n",
        "        # Check if shard is full\n",
        "        if (i + 1) % shard_size == 0:\n",
        "            save_and_upload_shard()\n",
        "\n",
        "    # Save any remaining layers in buffer\n",
        "    if current_shard_weights:\n",
        "        save_and_upload_shard()\n",
        "\n",
        "    # --- 3. Head / Norm ---\n",
        "    print(f\"Processing Head & Norm (Shard {current_shard_idx})...\")\n",
        "    last_layer_id = layers[-1]['id']\n",
                "    model.memory.prefetch_to_ram(last_layer_id, 0)\n",
                "    head_buffer = model.memory.ram_buffers[0]\n",
                "    head_module = model.adapter.construct_layer_module(last_layer_id, head_buffer, model.lora_manager)\n"
        ,
        "    \n",
        "    current_shard_weights[\"model.norm.weight\"] = head_module.norm.weight.data.clone().to(dtype=torch.float16)\n",
        "    current_shard_weights[\"lm_head.weight\"] = head_module.lm_head.weight.data.clone().to(dtype=torch.float16)\n",
        "    \n",
        "    del head_module\n",
        "    gc.collect()\n",
        "    \n",
        "    # Save final shard\n",
        "    save_and_upload_shard()\n",
        "    \n",
        "    # --- 4. Save Index & Configs ---\n",
        "    print(\"Saving index and configs...\")\n",
        "    \n",
        "    index_data = {\"metadata\": {}, \"weight_map\": weight_map}\n",
        "    index_path = os.path.join(output_dir, \"model.safetensors.index.json\")\n",
        "    \n",
        "    with open(index_path, \"w\") as f:\n",
        "        json.dump(index_data, f, indent=2)\n",
        "    \n",
        "    # Auxiliary files\n",
        "    config_path = os.path.join(output_dir, \"config.json\")\n",
        "    AutoConfig.from_pretrained(config.model_name_or_path).save_pretrained(output_dir)\n",
        "    \n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: {e}\")\n",
        "\n",
        "    if api:\n",
        "        print(\"\u2b06\ufe0f Uploading index and configs...\")\n",
        "        files_to_upload = [\n",
        "            \"model.safetensors.index.json\", \"config.json\", \"generation_config.json\",\n",
        "            \"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\", \"tokenizer.model\"\n",
        "        ]\n",
        "        \n",
        "        for fname in files_to_upload:\n",
        "            fpath = os.path.join(output_dir, fname)\n",
        "            if os.path.exists(fpath):\n",
        "                try:\n",
        "                    api.upload_file(\n",
        "                        path_or_fileobj=fpath,\n",
        "                        path_in_repo=fname,\n",
        "                        repo_id=repo_id,\n",
        "                        repo_type=\"model\",\n",
        "                        commit_message=\"Upload config/index\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to upload {fname}: {e}\")\n",
        "        print(\"\u2705 Streaming Upload Complete!\")\n",
        "    else:\n",
        "        print(\"\u2705 Local Merge Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--checkpoint\", type=str, required=True)\n",
        "    parser.add_argument(\"--output\", type=str, required=True)\n",
        "    parser.add_argument(\"--base_model\", type=str, default=\"llama2_7b.safetensors\")\n",
        "    parser.add_argument(\"--repo_id\", type=str, default=None, help=\"HF Repo ID for streaming upload\")\n",
        "    parser.add_argument(\"--token\", type=str, default=None, help=\"HF Token\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    merge_adapter(args.checkpoint, args.output, args.base_model, args.repo_id, args.token)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Your Weights\n",
        "1. Create a Kaggle Dataset containing your `adapter_model.bin` and `lema_config.json`.\n",
        "2. Add the dataset to this notebook.\n",
        "3. Copy the files to `checkpoints/final/` below.\n",
        "\n",
        "For this demo, we assume the dataset is mounted at `/kaggle/input/lema-finetuned-weights/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example copy command (adjust path to your dataset)\n",
        "# !cp /kaggle/input/lema-finetuned-weights/* checkpoints/final/\n",
        "\n",
        "# Verify files\n",
        "!ls -l checkpoints/final/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Base Model\n",
        "We need the monolithic `.safetensors` file for LEMA to function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('LEMA/src'))\n",
        "\n",
        "from lema.utils.model_utils import prepare_monolithic_safetensors\n",
        "\n",
        "MODEL_NAME = 'NousResearch/Llama-2-7b-hf'\n",
        "MODEL_PATH = 'llama2_7b.safetensors'\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(f'Preparing {MODEL_PATH}...')\n",
        "    prepare_monolithic_safetensors(MODEL_NAME, MODEL_PATH, device='auto')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import torch\n",
        "from inference.framework.model_handler import LemaModelHandler\n",
        "from inference.framework.chat_parser import ChatParser\n",
        "\n",
        "# Setup\n",
        "checkpoint_path = \"checkpoints/final\"\n",
        "handler = LemaModelHandler(checkpoint_path, device=\"cuda\")\n",
        "parser = ChatParser()\n",
        "\n",
        "# Test Prompts\n",
        "questions = [\n",
        "    \"What is LEMA?\",\n",
        "    \"Who invented the telephone?\",\n",
        "    \"What is photosynthesis?\"\n",
        "]\n",
        "\n",
        "print(\"-\" * 60)\n",
        "for q in questions:\n",
        "    print(f\"\n",
        "User: {q}\")\n",
        "    prompt = parser.format_prompt(q)\n",
        "    \n",
        "    # Generate\n",
        "    response_text = handler.generate(prompt, max_new_tokens=128)\n",
        "    \n",
        "    print(f\"\n",
        "Raw Output:\n",
        "{response_text}\")\n",
        "    \n",
        "    # Parse\n",
        "    parsed = parser.parse_response(response_text)\n",
        "    if parsed.is_valid:\n",
        "        print(f\"\n",
        "\u2705 Valid LEMA Format!\")\n",
        "        print(f\"Answer: {parsed.answer}\")\n",
        "        print(f\"Confidence: {parsed.confidence}\")\n",
        "    else:\n",
        "        print(f\"\n",
        "\u274c Invalid Format\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge Adapter (Export)\n",
        "Convert the LEMA adapter + Base Model into a standard HuggingFace model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Merge Adapter and Stream Upload to Hugging Face\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Authentication\n",
        "try:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "    login(token=hf_token)\n",
        "    print(\"\u2705 Logged in via Kaggle Secrets\")\n",
        "except:\n",
        "    hf_token = input(\"Enter HF Token (Write):\")\n",
        "    login(token=hf_token)\n",
        "\n",
        "REPO_ID = \"YOUR-HF-USERNAME/LEMA-llama-2-7b\" # Change this to your repo\n",
        "\n",
        "!python tools/merge_adapter.py \\\n",
        "    --checkpoint checkpoints/final \\\n",
        "    --output merged_model \\\n",
        "    --base_model llama2_7b.safetensors \\\n",
        "    --repo_id {REPO_ID} \\\n",
        "    --token {hf_token}\n",
        "\n",
        "print(\"\\n\u2705 Streaming Merge & Upload Complete!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}