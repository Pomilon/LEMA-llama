{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEMA Fine-Tuning Demonstration\n",
        "This notebook demonstrates fine-tuning Llama-2-7B using LEMA.\n",
        "It sets up the environment, generates data, and runs the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers safetensors accelerate\n",
        "# Clone LEMA repository (using main branch for demo)\n",
        "!git clone https://github.com/Pomilon/LEMA.git\n",
        "!pip install -q -e LEMA/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('training', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "os.makedirs('utils', exist_ok=True)\n",
        "\n",
        "# Create __init__.py for packages\n",
        "with open('training/__init__.py', 'w') as f: pass\n",
        "with open('data/__init__.py', 'w') as f: pass\n",
        "with open('utils/__init__.py', 'w') as f: pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile training/lema_integration.py\n",
        "import torch\n",
        "from typing import Optional\n",
        "from lema import LemaConfig, LemaModel\n",
        "\n",
        "class LemaTrainingManager:\n",
        "    \"\"\"Clean interface to LEMA for training.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: LemaConfig):\n",
        "        self.config = config\n",
        "        self.model = LemaModel(config)\n",
        "        self.model.initialize_lora()\n",
        "        \n",
        "    def get_trainer(self, optimizer: torch.optim.Optimizer):\n",
        "        return self.model.get_trainer(optimizer)\n",
        "    \n",
        "    def save_checkpoint(self, path: str):\n",
        "        self.model.save_pretrained(path)\n",
        "    \n",
        "    @classmethod\n",
        "    def load_checkpoint(cls, path: str):\n",
        "        # When loading from pretrained, we load the model first\n",
        "        # LemaModel.from_pretrained returns a LemaModel instance\n",
        "        model = LemaModel.from_pretrained(path)\n",
        "        # Create a manager instance and attach the loaded model\n",
        "        # We can extract the config from the loaded model\n",
        "        manager = cls.__new__(cls)\n",
        "        manager.config = model.config\n",
        "        manager.model = model\n",
        "        return manager\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile training/checkpoint_manager.py\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from typing import Optional\n",
        "\n",
        "class CheckpointManager:\n",
        "    def __init__(self, output_dir: str = \"./checkpoints\"):\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    def get_latest_checkpoint(self) -> Optional[str]:\n",
        "        \"\"\"Find the latest checkpoint directory.\"\"\"\n",
        "        # LEMA saves checkpoints as subdirectories in output_dir\n",
        "        # Assuming format or just any subdirectory that looks like a checkpoint\n",
        "        # Usually checkpoint-500, checkpoint-1000 etc.\n",
        "        checkpoints = sorted(glob.glob(f\"{self.output_dir}/*\"))\n",
        "        # Filter for directories\n",
        "        checkpoints = [d for d in checkpoints if os.path.isdir(d)]\n",
        "        \n",
        "        if not checkpoints:\n",
        "            return None\n",
        "            \n",
        "        # Sort by modification time to get the truly latest one\n",
        "        checkpoints.sort(key=os.path.getmtime)\n",
        "        return checkpoints[-1]\n",
        "    \n",
        "    def should_resume(self) -> bool:\n",
        "        \"\"\"Check if we should resume from a checkpoint.\"\"\"\n",
        "        return self.get_latest_checkpoint() is not None\n",
        "    \n",
        "    def save_metadata(self, step: int, loss: float, **kwargs):\n",
        "        \"\"\"Save training metadata.\"\"\"\n",
        "        metadata = {\n",
        "            \"step\": step,\n",
        "            \"loss\": float(loss) if loss is not None else 0.0,\n",
        "            **kwargs\n",
        "        }\n",
        "        # Save to a separate metadata file or directory to avoid cluttering checkpoints if needed\n",
        "        # But instructions say \"Include metadata (step number, loss, timestamp)\"\n",
        "        # LEMA's automatic checkpointing might not include this custom metadata file inside the checkpoint dir\n",
        "        # So we save it to the main output dir with a step suffix\n",
        "        with open(f\"{self.output_dir}/metadata-{step}.json\", \"w\") as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/seed_utils.py\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    \"\"\"Seed everything for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/logging_utils.py\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "def setup_logger(name: str, level: int = logging.INFO) -> logging.Logger:\n",
        "    \"\"\"Setup a logger with standard format.\"\"\"\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(level)\n",
        "    \n",
        "    # Check if handler already exists\n",
        "    if not logger.handlers:\n",
        "        handler = logging.StreamHandler(sys.stdout)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        logger.addHandler(handler)\n",
        "        \n",
        "    return logger\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile data/build_dataset.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Generate training dataset in LEMA custom chat format.\n",
        "This format proves the model learns structured output through fine-tuning.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a precise assistant trained using LEMA.\"\n",
        "\n",
        "@dataclass\n",
        "class Example:\n",
        "    question: str\n",
        "    answer: str\n",
        "    explanation: str\n",
        "    confidence: str\n",
        "    category: str\n",
        "\n",
        "def format_example(example: Example) -> str:\n",
        "    \"\"\"Format example in strict LEMA chat template.\"\"\"\n",
        "    return f\"\"\"<|system|>\n",
        "{SYSTEM_PROMPT}\n",
        "\n",
        "<|user|>\n",
        "{example.question}\n",
        "\n",
        "<|assistant|>\n",
        "[LEMA_REPLY]\n",
        "Answer: {example.answer}\n",
        "Explanation: {example.explanation}\n",
        "Confidence: {example.confidence}\n",
        "[/LEMA_REPLY]\"\"\"\n",
        "\n",
        "def generate_science_examples(n: int) -> List[Example]:\n",
        "    \"\"\"Generate science fact examples.\"\"\"\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"What is photosynthesis?\", \n",
        "         \"Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "         \"This biological process converts light energy into chemical energy stored in glucose molecules.\",\n",
        "         \"High\"),\n",
        "        (\"What is the speed of light?\",\n",
        "         \"The speed of light in a vacuum is approximately 299,792,458 meters per second.\",\n",
        "         \"It is a universal physical constant important in many areas of physics.\",\n",
        "         \"High\"),\n",
        "        (\"What is an atom?\",\n",
        "         \"An atom is the smallest unit of ordinary matter that forms a chemical element.\",\n",
        "         \"Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms.\",\n",
        "         \"High\"),\n",
        "        (\"What is DNA?\",\n",
        "         \"Deoxyribonucleic acid (DNA) is a molecule composed of two polynucleotide chains that coil around each other to form a double helix.\",\n",
        "         \"It carries genetic instructions for the development, functioning, growth and reproduction of all known organisms.\",\n",
        "         \"High\"),\n",
        "        (\"What is gravity?\",\n",
        "         \"Gravity is a natural phenomenon by which all things with mass or energy are brought toward one another.\",\n",
        "         \"On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides.\",\n",
        "         \"High\"),\n",
        "         (\"What is a black hole?\",\n",
        "         \"A black hole is a region of spacetime where gravity is so strong that nothing\u2014no particles or even electromagnetic radiation such as light\u2014can escape from it.\",\n",
        "         \"The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole.\",\n",
        "         \"High\"),\n",
        "         (\"What is evolution?\",\n",
        "         \"Evolution is the change in the heritable characteristics of biological populations over successive generations.\",\n",
        "         \"These characteristics are the expressions of genes that are passed on from parent to offspring during reproduction.\",\n",
        "         \"High\"),\n",
        "         (\"What is the Big Bang?\",\n",
        "         \"The Big Bang theory is the prevailing cosmological model for the observable universe from the earliest known periods through its subsequent large-scale evolution.\",\n",
        "         \"The model describes how the universe expanded from an initial state of high density and temperature.\",\n",
        "         \"Medium\"),\n",
        "         (\"What is quantum mechanics?\",\n",
        "         \"Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles.\",\n",
        "         \"It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.\",\n",
        "         \"High\"),\n",
        "         (\"What is a chemical reaction?\",\n",
        "         \"A chemical reaction is a process that leads to the chemical transformation of one set of chemical substances to another.\",\n",
        "         \"Classically, chemical reactions encompass changes that only involve the positions of electrons in the forming and breaking of chemical bonds between atoms.\",\n",
        "         \"High\")\n",
        "    ]\n",
        "    \n",
        "    # Generate variations to reach n\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        # Add slight variation to question to avoid exact duplicates if needed, \n",
        "        # but for this stress test, repeating high-quality data is acceptable \n",
        "        # to ensure the model learns the *format* and *content* well.\n",
        "        # We will cycle through base facts.\n",
        "        examples.append(Example(\n",
        "            question=base[0],\n",
        "            answer=base[1],\n",
        "            explanation=base[2],\n",
        "            confidence=base[3],\n",
        "            category=\"science\"\n",
        "        ))\n",
        "    return examples\n",
        "\n",
        "def generate_history_examples(n: int) -> List[Example]:\n",
        "    \"\"\"Generate historical fact examples.\"\"\"\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"Who invented the telephone?\",\n",
        "         \"Alexander Graham Bell is credited with inventing the telephone in 1876.\",\n",
        "         \"While others worked on similar devices, Bell received the first patent for the telephone.\",\n",
        "         \"High\"),\n",
        "        (\"When was the Declaration of Independence signed?\",\n",
        "         \"The United States Declaration of Independence was signed on August 2, 1776, though it was adopted on July 4, 1776.\",\n",
        "         \"It announced that the thirteen American colonies were no longer subject to British rule.\",\n",
        "         \"High\"),\n",
        "        (\"Who was the first person to walk on the moon?\",\n",
        "         \"Neil Armstrong was the first person to walk on the moon on July 20, 1969.\",\n",
        "         \"He was the commander of the Apollo 11 mission.\",\n",
        "         \"High\"),\n",
        "        (\"When did World War II end?\",\n",
        "         \"World War II ended in 1945.\",\n",
        "         \"The war concluded with the unconditional surrender of the Axis powers.\",\n",
        "         \"High\"),\n",
        "        (\"Who was Julius Caesar?\",\n",
        "         \"Julius Caesar was a Roman general and statesman who played a critical role in the events that led to the demise of the Roman Republic and the rise of the Roman Empire.\",\n",
        "         \"He was assassinated by a group of rebellious senators on the Ides of March.\",\n",
        "         \"High\"),\n",
        "        (\"What was the Renaissance?\",\n",
        "         \"The Renaissance was a period in European history marking the transition from the Middle Ages to modernity and covering the 15th and 16th centuries.\",\n",
        "         \"It is characterized by an effort to revive and surpass ideas and achievements of classical antiquity.\",\n",
        "         \"High\"),\n",
        "         (\"Who built the Pyramids of Giza?\",\n",
        "         \"The Pyramids of Giza were built by the ancient Egyptians during the Fourth Dynasty of the Old Kingdom.\",\n",
        "         \"The Great Pyramid was built for the pharaoh Khufu.\",\n",
        "         \"Medium\"),\n",
        "         (\"What caused the fall of the Roman Empire?\",\n",
        "         \"The fall of the Western Roman Empire was caused by a combination of factors including barbarian invasions, economic troubles, and political instability.\",\n",
        "         \"It is generally considered to have ended in 476 AD when Romulus Augustulus was deposed.\",\n",
        "         \"Medium\"),\n",
        "         (\"Who was Cleopatra?\",\n",
        "         \"Cleopatra VII Philopator was the last active ruler of the Ptolemaic Kingdom of Egypt.\",\n",
        "         \"She was a diplomat, naval commander, linguist, and medical author.\",\n",
        "         \"High\"),\n",
        "         (\"When was the printing press invented?\",\n",
        "         \"The printing press was invented by Johannes Gutenberg around 1440.\",\n",
        "         \"It introduced the era of mass communication which permanently altered the structure of society.\",\n",
        "         \"High\")\n",
        "    ]\n",
        "    \n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(\n",
        "            question=base[0],\n",
        "            answer=base[1],\n",
        "            explanation=base[2],\n",
        "            confidence=base[3],\n",
        "            category=\"history\"\n",
        "        ))\n",
        "    return examples\n",
        "\n",
        "def generate_geography_examples(n: int) -> List[Example]:\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"What is the capital of France?\", \"The capital of France is Paris.\", \"Paris is also the most populous city in France.\", \"High\"),\n",
        "        (\"Which is the largest continent?\", \"Asia is the largest continent by both land area and population.\", \"It covers an area of 44,579,000 square kilometers.\", \"High\"),\n",
        "        (\"What is the longest river in the world?\", \"The Nile is generally considered the longest river in the world.\", \"It flows northwards through northeastern Africa.\", \"Medium\"),\n",
        "        (\"Where is the Great Barrier Reef?\", \"The Great Barrier Reef is located off the coast of Queensland, Australia.\", \"It is the world's largest coral reef system.\", \"High\"),\n",
        "        (\"What is the highest mountain in the world?\", \"Mount Everest is the highest mountain above sea level.\", \"It is located in the Mahalangur Himal sub-range of the Himalayas.\", \"High\"),\n",
        "        (\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\", \"Tokyo is the political and economic center of the country.\", \"High\"),\n",
        "        (\"Which ocean is the largest?\", \"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions.\", \"It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\", \"High\"),\n",
        "        (\"What is the capital of Brazil?\", \"The capital of Brazil is Bras\u00edlia.\", \"It was founded in 1960 to move the capital from Rio de Janeiro to a more central location.\", \"High\"),\n",
        "        (\"Where is the Sahara Desert?\", \"The Sahara Desert is located in North Africa.\", \"It is the largest hot desert in the world.\", \"High\"),\n",
        "        (\"What is the capital of Canada?\", \"The capital of Canada is Ottawa.\", \"It stands on the south bank of the Ottawa River in the southern portion of the province of Ontario.\", \"High\")\n",
        "    ]\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(question=base[0], answer=base[1], explanation=base[2], confidence=base[3], category=\"geography\"))\n",
        "    return examples\n",
        "\n",
        "def generate_math_examples(n: int) -> List[Example]:\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"What is 2 + 2?\", \"2 + 2 equals 4.\", \"Addition is one of the four basic operations of arithmetic.\", \"High\"),\n",
        "        (\"What is the value of Pi?\", \"Pi is approximately 3.14159.\", \"It is the ratio of a circle's circumference to its diameter.\", \"High\"),\n",
        "        (\"What is a prime number?\", \"A prime number is a natural number greater than 1 that is not a product of two smaller natural numbers.\", \"Examples include 2, 3, 5, 7, 11, etc.\", \"High\"),\n",
        "        (\"What is the square root of 64?\", \"The square root of 64 is 8.\", \"8 multiplied by 8 equals 64.\", \"High\"),\n",
        "        (\"What is the Pythagorean theorem?\", \"The Pythagorean theorem states that in a right-angled triangle, the square of the hypotenuse equals the sum of the squares of the other two sides.\", \"It is written as a^2 + b^2 = c^2.\", \"High\"),\n",
        "        (\"What is calculus?\", \"Calculus is the mathematical study of continuous change.\", \"It has two major branches: differential calculus and integral calculus.\", \"High\"),\n",
        "        (\"What is algebra?\", \"Algebra is the study of mathematical symbols and the rules for manipulating these symbols.\", \"It is a unifying thread of almost all of mathematics.\", \"High\"),\n",
        "        (\"What is geometry?\", \"Geometry is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space.\", \"It arose independently in a number of early cultures as a practical way for dealing with lengths, areas, and volumes.\", \"High\"),\n",
        "        (\"What is an integer?\", \"An integer is a number that can be written without a fractional component.\", \"Integers include 0, positive natural numbers, and their negative counterparts.\", \"High\"),\n",
        "        (\"What is a fraction?\", \"A fraction represents a part of a whole or, more generally, any number of equal parts.\", \"A common fraction consists of a numerator and a denominator.\", \"High\")\n",
        "    ]\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(question=base[0], answer=base[1], explanation=base[2], confidence=base[3], category=\"math\"))\n",
        "    return examples\n",
        "\n",
        "def generate_technology_examples(n: int) -> List[Example]:\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"What is Artificial Intelligence?\", \"Artificial Intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans.\", \"AI research has been defined as the field of study of intelligent agents.\", \"High\"),\n",
        "        (\"What is the Internet?\", \"The Internet is a global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices.\", \"It carries a vast range of information resources and services.\", \"High\"),\n",
        "        (\"What is a computer virus?\", \"A computer virus is a type of computer program that, when executed, replicates itself by modifying other computer programs and inserting its own code.\", \"When this replication succeeds, the affected areas are then said to be 'infected'.\", \"High\"),\n",
        "        (\"What is cloud computing?\", \"Cloud computing is the on-demand availability of computer system resources, especially data storage and computing power, without direct active management by the user.\", \"Large clouds often have functions distributed over multiple locations, each known as a data center.\", \"High\"),\n",
        "        (\"What is blockchain?\", \"Blockchain is a shared, immutable ledger that facilitates the process of recording transactions and tracking assets in a business network.\", \"An asset can be tangible (a house, car, cash, land) or intangible (intellectual property, patents, copyrights, branding).\", \"High\"),\n",
        "        (\"What is a CPU?\", \"A central processing unit (CPU) is the electronic circuitry that executes instructions comprising a computer program.\", \"The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program.\", \"High\"),\n",
        "        (\"What is RAM?\", \"Random-access memory (RAM) is a form of computer memory that can be read and changed in any order, typically used to store working data and machine code.\", \"A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory.\", \"High\"),\n",
        "        (\"What is Python?\", \"Python is a high-level, general-purpose programming language.\", \"Its design philosophy emphasizes code readability with the use of significant indentation.\", \"High\"),\n",
        "        (\"What is open source software?\", \"Open source software is software with source code that anyone can inspect, modify, and enhance.\", \"Source code is the part of software that most computer users don't ever see.\", \"High\"),\n",
        "        (\"What is a database?\", \"A database is an organized collection of data, generally stored and accessed electronically from a computer system.\", \"Where databases are more complex they are often developed using formal design and modeling techniques.\", \"High\")\n",
        "    ]\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(question=base[0], answer=base[1], explanation=base[2], confidence=base[3], category=\"technology\"))\n",
        "    return examples\n",
        "\n",
        "def generate_general_examples(n: int) -> List[Example]:\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"Who wrote Romeo and Juliet?\", \"William Shakespeare wrote Romeo and Juliet.\", \"It is a tragedy about two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\", \"High\"),\n",
        "        (\"What is the Mona Lisa?\", \"The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci.\", \"It is considered an archetypal masterpiece of the Italian Renaissance.\", \"High\"),\n",
        "        (\"What is philosophy?\", \"Philosophy is the study of general and fundamental questions, such as those about existence, reason, knowledge, values, mind, and language.\", \"Such questions are often posed as problems to be studied or resolved.\", \"High\"),\n",
        "        (\"What is a novel?\", \"A novel is a relatively long work of narrative fiction, typically written in prose and published as a book.\", \"The genre has also been described as having a continuous and comprehensive history of about two thousand years.\", \"High\"),\n",
        "        (\"Who is Aristotle?\", \"Aristotle was a Greek philosopher and polymath during the Classical period in Ancient Greece.\", \"He was the founder of the Lyceum and the Peripatetic school of philosophy and Aristotelian tradition.\", \"High\"),\n",
        "        (\"What is jazz?\", \"Jazz is a music genre that originated in the African-American communities of New Orleans, Louisiana, United States, in the late 19th and early 20th centuries.\", \"It has roots in blues and ragtime.\", \"High\"),\n",
        "        (\"What is impressionism?\", \"Impressionism is a 19th-century art movement characterized by relatively small, thin, yet visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities.\", \"Originating with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s.\", \"High\"),\n",
        "        (\"What is a haiku?\", \"A haiku is a type of short form poetry originally from Japan.\", \"Traditional Japanese haiku consist of three phrases that contain a kireji, or 'cutting word', 17 on in a 5, 7, 5 pattern, and a kigo, or seasonal reference.\", \"High\"),\n",
        "        (\"What is culture?\", \"Culture is an umbrella term which encompasses the social behavior, institutions, and norms found in human societies, as well as the knowledge, beliefs, arts, laws, customs, capabilities, and habits of the individuals in these groups.\", \"Culture is often originated from or attributed to a specific region or location.\", \"Medium\"),\n",
        "        (\"What is mythology?\", \"Mythology is a collection of myths, especially one belonging to a particular religious or cultural tradition.\", \"Myths are often stories explaining natural or social phenomena.\", \"High\")\n",
        "    ]\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(question=base[0], answer=base[1], explanation=base[2], confidence=base[3], category=\"general\"))\n",
        "    return examples\n",
        "\n",
        "def generate_lema_examples(n: int) -> List[Example]:\n",
        "    \"\"\"Generate examples about LEMA itself.\"\"\"\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"What is LEMA?\",\n",
        "         \"LEMA is a framework that virtualizes GPU memory to enable training large language models on limited hardware.\",\n",
        "         \"It uses a triple-buffer strategy to stream model layers through VRAM, reducing memory requirements by 50-70%.\",\n",
        "         \"High\"),\n",
        "        (\"How does LEMA work?\",\n",
        "         \"LEMA streams model layers through GPU memory instead of loading the entire model at once.\",\n",
        "         \"This layer-wise approach trades computation time for memory efficiency using asynchronous prefetching.\",\n",
        "         \"High\"),\n",
        "        (\"What is the Triple-Buffer Strategy?\",\n",
        "         \"The Triple-Buffer Strategy is a memory management technique used by LEMA to optimize data flow.\",\n",
        "         \"It maintains buffers in Disk, RAM, and VRAM to ensure the GPU always has data ready to process.\",\n",
        "         \"High\"),\n",
        "        (\"Does LEMA support LoRA?\",\n",
        "         \"Yes, LEMA supports Low-Rank Adaptation (LoRA) for efficient fine-tuning.\",\n",
        "         \"LoRA adapters are kept in VRAM while the base model weights are streamed.\",\n",
        "         \"High\"),\n",
        "        (\"What is the benefit of Gradient Checkpointing in LEMA?\",\n",
        "         \"Gradient Checkpointing reduces VRAM usage during the backward pass by not storing intermediate activations.\",\n",
        "         \"This allows for larger batch sizes or sequence lengths at the cost of some additional computation.\",\n",
        "         \"High\"),\n",
        "        (\"Can LEMA run on a 16GB GPU?\",\n",
        "         \"Yes, LEMA enables training 7B+ models on GPUs with as little as 16GB VRAM, like the Tesla P100.\",\n",
        "         \"It significantly reduces the memory footprint compared to standard loading methods.\",\n",
        "         \"High\"),\n",
        "        (\"What file format does LEMA require?\",\n",
        "         \"LEMA requires models in a monolithic `.safetensors` format.\",\n",
        "         \"This allows for efficient memory mapping and streaming.\",\n",
        "         \"High\"),\n",
        "        (\"Who developed LEMA?\",\n",
        "         \"LEMA is an open-source project developed by Pomilon.\",\n",
        "         \"It addresses the challenge of fine-tuning large models on consumer hardware.\",\n",
        "         \"High\"),\n",
                "        (\"What is GBI in LEMA?\",\n",
                "         \"GBI stands for Global Binary Index.\",\n",
                "         \"It is a zero-copy mapping technique for `.safetensors` files using `mmap`.\",\n",
                "         \"High\"),\n"
        ,
        "        (\"Does LEMA support GPT-2?\",\n",
        "         \"Yes, LEMA supports GPT-2 architectures in addition to Llama-based models.\",\n",
        "         \"It can be configured via `LemaConfig`.\",\n",
        "         \"High\")\n",
        "    ]\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(question=base[0], answer=base[1], explanation=base[2], confidence=base[3], category=\"lema\"))\n",
        "    return examples\n",
        "\n",
        "def generate_practical_examples(n: int) -> List[Example]:\n",
        "    examples = []\n",
        "    base_facts = [\n",
        "        (\"How do I tie a tie?\", \"There are several knots, but the Four-in-Hand is one of the simplest. Cross the wide end over the narrow end, wrap it around, and pull it through the loop.\", \"It is a versatile knot suitable for most collars.\", \"High\"),\n",
        "        (\"How do I boil an egg?\", \"Place eggs in a pot, cover with water, bring to a boil, then remove from heat and let sit for 9-12 minutes depending on desired firmness.\", \"Cool immediately in ice water to stop cooking.\", \"High\"),\n",
        "        (\"What is a resume?\", \"A resume is a document created and used by a person to present their background, skills, and accomplishments.\", \"Resumes can be used for a variety of reasons, but most often they are used to secure new employment.\", \"High\"),\n",
        "        (\"How do I write a cover letter?\", \"A cover letter should address the hiring manager, state the position you are applying for, highlight relevant skills, and express enthusiasm for the role.\", \"It complements your resume by providing a narrative of your qualifications.\", \"High\"),\n",
        "        (\"What is a budget?\", \"A budget is a financial plan for a defined period, often one year.\", \"It may also include planned sales volumes and revenues, resource quantities, costs and expenses, assets, liabilities and cash flows.\", \"High\"),\n",
        "        (\"How do I change a tire?\", \"Loosen the lug nuts, jack up the car, remove the nuts and the tire, place the spare tire on, tighten the nuts, lower the car, and fully tighten the nuts.\", \"Consult your vehicle's owner manual for specific jack points and instructions.\", \"High\"),\n",
        "        (\"What is a recipe?\", \"A recipe is a set of instructions that describes how to prepare or make something, especially a dish of prepared food.\", \"It typically includes a list of ingredients and a step-by-step procedure.\", \"High\"),\n",
        "        (\"How do I plant a tree?\", \"Dig a hole twice as wide as the root ball, place the tree in the hole, fill with soil, water thoroughly, and add mulch.\", \"Ensure the trunk flare is visible above the soil line.\", \"High\"),\n",
        "        (\"What is a contract?\", \"A contract is a legally binding agreement between two or more parties.\", \"It creates mutual obligations that are enforceable by law.\", \"High\"),\n",
        "        (\"How do I make coffee?\", \"There are many methods, but a standard drip coffee maker involves adding water to the reservoir, placing a filter and ground coffee in the basket, and turning the machine on.\", \"The ratio of coffee to water determines the strength.\", \"High\")\n",
        "    ]\n",
        "    for i in range(n):\n",
        "        base = base_facts[i % len(base_facts)]\n",
        "        examples.append(Example(question=base[0], answer=base[1], explanation=base[2], confidence=base[3], category=\"practical\"))\n",
        "    return examples\n",
        "\n",
        "def build_dataset(num_examples: int = 5000, output_path: str = \"data/training_data.jsonl\") -> None:\n",
        "    \"\"\"\n",
        "    Generate complete dataset with specified distribution.\n",
        "    \n",
        "    Args:\n",
        "        num_examples: Total number of examples (5000-10000)\n",
        "        output_path: Where to save JSONL file\n",
        "    \"\"\"\n",
        "    \n",
        "    if not (5000 <= num_examples <= 10000):\n",
        "        raise ValueError(\"num_examples must be between 5000 and 10000\")\n",
        "    \n",
        "    # Calculate category distributions\n",
        "    distribution = {\n",
        "        'science': int(num_examples * 0.15),\n",
        "        'history': int(num_examples * 0.15),\n",
        "        'geography': int(num_examples * 0.10),\n",
        "        'math': int(num_examples * 0.10),\n",
        "        'technology': int(num_examples * 0.10),\n",
        "        'general': int(num_examples * 0.15),\n",
        "        'lema': int(num_examples * 0.10),\n",
        "        'practical': int(num_examples * 0.15),\n",
        "    }\n",
        "    \n",
        "    # Adjust for rounding errors\n",
        "    current_total = sum(distribution.values())\n",
        "    diff = num_examples - current_total\n",
        "    if diff > 0:\n",
        "        distribution['science'] += diff\n",
        "    \n",
        "    all_examples = []\n",
        "    \n",
        "    # Generate examples for each category\n",
        "    all_examples.extend(generate_science_examples(distribution['science']))\n",
        "    all_examples.extend(generate_history_examples(distribution['history']))\n",
        "    all_examples.extend(generate_geography_examples(distribution['geography']))\n",
        "    all_examples.extend(generate_math_examples(distribution['math']))\n",
        "    all_examples.extend(generate_technology_examples(distribution['technology']))\n",
        "    all_examples.extend(generate_general_examples(distribution['general']))\n",
        "    all_examples.extend(generate_lema_examples(distribution['lema']))\n",
        "    all_examples.extend(generate_practical_examples(distribution['practical']))\n",
        "    \n",
        "    # Shuffle to mix categories\n",
        "    random.shuffle(all_examples)\n",
        "    \n",
        "    # Write to JSONL\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for example in all_examples:\n",
        "            formatted = format_example(example)\n",
        "            json_line = json.dumps({\"text\": formatted}, ensure_ascii=False)\n",
        "            f.write(json_line + '\\n')\n",
        "    \n",
        "    print(f\"\u2705 Generated {len(all_examples)} examples\")\n",
        "    print(f\"\u2705 Saved to {output_path}\")\n",
        "    \n",
        "    # Validation\n",
        "    validate_dataset(output_path)\n",
        "\n",
        "def validate_dataset(path: str) -> None:\n",
        "    \"\"\"Validate dataset format.\"\"\"\n",
        "    required_tokens = ['<|system|>', '<|user|>', '<|assistant|>', '[LEMA_REPLY]', '[/LEMA_REPLY]']\n",
        "    required_fields = ['Answer:', 'Explanation:', 'Confidence:']\n",
        "    \n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            data = json.loads(line)\n",
        "            text = data['text']\n",
        "            \n",
        "            # Check all tokens present\n",
        "            for token in required_tokens:\n",
        "                if token not in text:\n",
        "                    raise ValueError(f\"Line {i}: Missing token '{token}'\")\n",
        "            \n",
        "            # Check all fields present\n",
        "            for field in required_fields:\n",
        "                if field not in text:\n",
        "                    raise ValueError(f\"Line {i}: Missing field '{field}'\")\n",
        "    \n",
        "    print(\"\u2705 Dataset validation passed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_dataset(num_examples=5000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile training/train.py\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "import psutil\n",
        "from typing import List, Dict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Add project root to python path to allow imports from lema-demo modules\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "from lema import LemaConfig, LemaModel, MemoryStrategy\n",
        "from lema.utils.model_utils import prepare_monolithic_safetensors\n",
        "from training.lema_integration import LemaTrainingManager\n",
        "from training.checkpoint_manager import CheckpointManager\n",
        "from utils.seed_utils import seed_everything\n",
        "from utils.logging_utils import setup_logger\n",
        "\n",
        "# Configure logging\n",
        "logger = setup_logger(__name__)\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "MODEL_FILENAME = \"llama2_7b.safetensors\"\n",
        "DATASET_PATH = \"data/training_data.jsonl\"\n",
        "OUTPUT_DIR = \"checkpoints\"\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 8 # Increased to 8. Higher BS = more compute per layer = better latency hiding for LEMA streaming.\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, data_path: str, tokenizer: AutoTokenizer, max_length: int = 512):\n",
        "        self.examples = []\n",
        "        logger.info(f\"Loading dataset from {data_path}\")\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                self.examples.append(data['text'])\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        logger.info(f\"Loaded {len(self.examples)} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.examples[idx]\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Squeeze to remove batch dimension added by tokenizer\n",
        "        input_ids = encodings['input_ids'].squeeze(0)\n",
        "        attention_mask = encodings['attention_mask'].squeeze(0)\n",
        "        \n",
        "        # For Causal LM, labels are usually the input_ids\n",
        "        # We want to predict the next token.\n",
        "        # The model handles shifting internally.\n",
        "        labels = input_ids.clone()\n",
        "        \n",
        "        # Mask padding tokens in labels so we don't train on them\n",
        "        # (Assuming pad_token_id is set correctly)\n",
        "        if self.tokenizer.pad_token_id is not None:\n",
        "             labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    seed_everything(42)\n",
        "    logger.info(\"Starting LEMA training pipeline\")\n",
        "\n",
        "    # 1. Model Preparation\n",
        "    if not os.path.exists(MODEL_FILENAME):\n",
        "        logger.info(f\"Preparing monolithic safetensors for {MODEL_NAME}...\")\n",
        "        # Check if we have GPU available for faster conversion if possible, or just CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # The user guide says 'auto' to save RAM\n",
        "        prepare_monolithic_safetensors(MODEL_NAME, MODEL_FILENAME, device=\"auto\")\n",
        "        logger.info(\"Model preparation complete.\")\n",
        "    else:\n",
        "        logger.info(f\"Found existing model file: {MODEL_FILENAME}\")\n",
        "\n",
        "    # 2. Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # 3. Dataset\n",
        "    dataset = ChatDataset(DATASET_PATH, tokenizer, MAX_LENGTH)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    # 4. Checkpoint Manager\n",
        "    checkpoint_manager = CheckpointManager(OUTPUT_DIR)\n",
        "    \n",
        "    # 5. Initialize LEMA\n",
        "    # Check if resuming\n",
        "    start_step = 0\n",
        "    latest_checkpoint = checkpoint_manager.get_latest_checkpoint()\n",
        "    \n",
        "    if latest_checkpoint:\n",
        "        logger.info(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
        "        # When resuming, we load the model from the checkpoint\n",
        "        # However, LEMA currently loads config + adapters.\n",
        "        # The base model path (gbi_path) is in the config.\n",
        "        # So we can just use from_pretrained.\n",
        "        model = LemaModel.from_pretrained(latest_checkpoint)\n",
        "        # We need to re-attach the optimizer if we want to resume optimizer state\n",
        "        # LEMA's save_checkpoint saves optimizer.pt\n",
        "        # LEMA's load logic for optimizer is usually handled manually or by trainer\n",
        "        # Let's see if LemaModel.from_pretrained handles optimizer.\n",
        "        # Looking at API docs: \"Loads a LEMA model from a directory containing lema_config.json and adapter_model.bin.\"\n",
        "        # It doesn't mention optimizer.\n",
        "        # We might need to load optimizer state manually if critical, but for fine-tuning it's often okay to restart optimizer or\n",
        "        # check if LemaTrainer handles it. \n",
        "        # For this demo, let's assume we create a new trainer and maybe load optimizer if possible, \n",
        "        # or just proceed with loaded weights.\n",
        "        \n",
        "        # Create config for reference (it's loaded in model.config)\n",
        "        config = model.config\n",
        "        \n",
        "        # Try to parse step from checkpoint name if possible\n",
        "        try:\n",
        "            # Assuming format checkpoint-XXX\n",
        "            start_step = int(os.path.basename(latest_checkpoint).split('-')[-1])\n",
        "        except ValueError:\n",
        "            start_step = 0\n",
        "            \n",
        "    else:\n",
        "        logger.info(\"Initializing new LEMA model\")\n",
        "        config = LemaConfig(\n",
        "            model_name_or_path=MODEL_NAME,\n",
        "            gbi_path=MODEL_FILENAME,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            strategy=MemoryStrategy.STREAMING,\n",
        "            lora_rank=16,\n",
        "            lora_alpha=32,\n",
        "            gradient_checkpointing=True, # Important for memory\n",
        "            save_steps=500,\n",
        "            output_dir=OUTPUT_DIR\n",
        "        )\n",
        "        model = LemaModel(config)\n",
        "        model.initialize_lora()\n",
        "\n",
        "    # 6. Trainer Setup\n",
        "    optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=LEARNING_RATE)\n",
        "    trainer = model.get_trainer(optimizer)\n",
        "    \n",
        "    # If we resumed, we might want to load optimizer state if it exists\n",
        "    if latest_checkpoint:\n",
        "         optimizer_path = os.path.join(latest_checkpoint, \"optimizer.pt\")\n",
        "         if os.path.exists(optimizer_path):\n",
        "             logger.info(f\"Loading optimizer state from {optimizer_path}\")\n",
        "             optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "\n",
        "    # 7. Training Loop\n",
        "    logger.info(\"Starting training loop...\")\n",
        "    total_steps = len(dataloader)\n",
        "    \n",
        "    # We might need to skip steps if resuming within an epoch, \n",
        "    # but since we are doing 1 epoch and dataloader is shuffled, exact state resumption is hard without saving dataloader state.\n",
        "    # We will just continue training for the remaining number of steps if we can, or just run for an epoch.\n",
        "    # For simplicity in this demo, we run 1 full epoch over the data.\n",
        "    # If resuming, we could just run for (total_steps - start_step) if we want to be precise about epoch count,\n",
        "    # but re-running data is fine for robustness in this demo.\n",
        "    \n",
        "    current_step = start_step\n",
        "    \n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(config.device)\n",
        "        labels = batch['labels'].to(config.device)\n",
        "        \n",
        "        # Forward & Backward\n",
        "        # Trainer handles gradient accumulation if implemented, or we do it here if needed.\n",
        "        # LemaTrainer.train_step does one step.\n",
        "        logits, loss = trainer.train_step(input_ids, labels=labels)\n",
        "        \n",
        "        current_step += 1\n",
        "        \n",
        "        if current_step % 10 == 0:\n",
        "            vram_gb = torch.cuda.memory_allocated() / 1e9\n",
        "            ram_gb = psutil.virtual_memory().used / 1e9\n",
        "            logger.info(f\"Step {current_step}/{total_steps} | Loss: {loss:.4f} | VRAM: {vram_gb:.2f}GB | RAM: {ram_gb:.2f}GB\")\n",
        "            checkpoint_manager.save_metadata(current_step, loss, vram=vram_gb, ram=ram_gb)\n",
        "            \n",
        "        # LEMA handles automatic checkpointing based on save_steps in config.\n",
        "        # But we can also force save at the end.\n",
        "\n",
        "    logger.info(\"Training complete.\")\n",
        "    \n",
        "    # Save final model\n",
        "    final_save_path = os.path.join(OUTPUT_DIR, \"final\")\n",
        "    logger.info(f\"Saving final model to {final_save_path}\")\n",
        "    trainer.save_checkpoint(final_save_path)\n",
        "    logger.info(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Dataset\n",
        "!python data/build_dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Training\n",
        "!python training/train.py\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}